{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-lighter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "martial-firewall",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Sarrera: Proiektuarena aurkezpena <a name=\"sarrera\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-jordan",
   "metadata": {
    "tags": []
   },
   "source": [
    "Aqui deberia de ir texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fancy-costs",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "pd.options.mode.chained_assignment = None \n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import missingno as msg\n",
    "import string\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split\n",
    "from spellchecker import SpellChecker\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn import preprocessing\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from imageio import imread"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functional-arabic",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-kingston",
   "metadata": {},
   "source": [
    "# Datuak <a name=\"datuak\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "final-fusion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OriginalTweet</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>@MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>advice Talk to your neighbours family to excha...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Coronavirus Australia: Woolworths to give elde...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>My food stock is not the only one which is emp...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Me, ready to go at supermarket during the #COV...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41152</th>\n",
       "      <td>Airline pilots offering to stock supermarket s...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41153</th>\n",
       "      <td>Response to complaint not provided citing COVI...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41154</th>\n",
       "      <td>You know its getting tough when @KameronWilds...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41155</th>\n",
       "      <td>Is it wrong that the smell of hand sanitizer i...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41156</th>\n",
       "      <td>@TartiiCat Well new/used Rift S are going for ...</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>41157 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           OriginalTweet Sentiment\n",
       "0      @MeNyrbie @Phil_Gahan @Chrisitv https://t.co/i...   Neutral\n",
       "1      advice Talk to your neighbours family to excha...  Positive\n",
       "2      Coronavirus Australia: Woolworths to give elde...  Positive\n",
       "3      My food stock is not the only one which is emp...  Positive\n",
       "4      Me, ready to go at supermarket during the #COV...  Negative\n",
       "...                                                  ...       ...\n",
       "41152  Airline pilots offering to stock supermarket s...   Neutral\n",
       "41153  Response to complaint not provided citing COVI...  Negative\n",
       "41154  You know its getting tough when @KameronWilds...  Positive\n",
       "41155  Is it wrong that the smell of hand sanitizer i...   Neutral\n",
       "41156  @TartiiCat Well new/used Rift S are going for ...  Negative\n",
       "\n",
       "[41157 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"Corona_NLP_train.csv\",engine='python')\n",
    "df.drop(columns={'UserName','ScreenName','Location','TweetAt'},axis=1,inplace=True)\n",
    "df['Sentiment'] = df['Sentiment'].replace(['Extremely Negative'],'Negative')\n",
    "df['Sentiment'] = df['Sentiment'].replace(['Extremely Positive'],'Positive')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "patient-mathematics",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41157, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "checked-growing",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "paperback-engine",
   "metadata": {},
   "source": [
    "### 1. URL-ak ezabatu\n",
    "URL-ak ez zaizkigu interesatzen, kenduko ditugu REGEX-erabiliz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fuzzy-leisure",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub(r'', text)\n",
    "df.drop[]\n",
    "df[\"OriginalTweet\"] = df[\"OriginalTweet\"].apply(lambda text: remove_urls(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-consumer",
   "metadata": {},
   "source": [
    "### 5. Minuskulara dena igaro\n",
    "\n",
    "Do eta do hitz berdinak dira"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "anonymous-iceland",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"OriginalTweet\"] = df[\"OriginalTweet\"].str.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "refined-patrol",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 6. Geldiera hitzak kendu\n",
    "\n",
    "Geldiera hitzak ez dute ezer garrantzitzurik gaineratzen, eta kasu askotan, zarata sortu dezakete\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "enclosed-intranet",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i, me, my, myself, we, our, ours, ourselves, you, you're, you've, you'll, you'd, your, yours, yourself, yourselves, he, him, his, himself, she, she's, her, hers, herself, it, it's, its, itself, they, them, their, theirs, themselves, what, which, who, whom, this, that, that'll, these, those, am, is, are, was, were, be, been, being, have, has, had, having, do, does, did, doing, a, an, the, and, but, if, or, because, as, until, while, of, at, by, for, with, about, against, between, into, through, during, before, after, above, below, to, from, up, down, in, out, on, off, over, under, again, further, then, once, here, there, when, where, why, how, all, any, both, each, few, more, most, other, some, such, no, nor, not, only, own, same, so, than, too, very, s, t, can, will, just, don, don't, should, should've, now, d, ll, m, o, re, ve, y, ain, aren, aren't, couldn, couldn't, didn, didn't, doesn, doesn't, hadn, hadn't, hasn, hasn't, haven, haven't, isn, isn't, ma, mightn, mightn't, mustn, mustn't, needn, needn't, shan, shan't, shouldn, shouldn't, wasn, wasn't, weren, weren't, won, won't, wouldn, wouldn't\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\", \".join(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "canadian-honduras",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words('english'))\n",
    "STOPWORDS.add('u')\n",
    "def remove_stopwords(text):\n",
    "    \"\"\"custom function to remove the stopwords\"\"\"\n",
    "    return \" \".join([word for word in str(text).split() if word not in set(stopwords.words('english'))])\n",
    "\n",
    "df[\"OriginalTweet\"] = df[\"OriginalTweet\"].apply(lambda text:remove_stopwords(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlimited-tragedy",
   "metadata": {},
   "source": [
    "### 7. Puntuazio markak kendu\n",
    "\n",
    "Puntuazio markak ez zaizkigu interesatzen, hori dela eta, kenduko ditugu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "rising-sharp",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panic food buying germany due #coronavirus begun. #organic left behind! #hamsterkauf panic buying called \"hamster purchases\"(hamsterkã¤ufeconfusion german, taken way hamsters stuff cheeks food.\n"
     ]
    }
   ],
   "source": [
    "print(df[\"OriginalTweet\"][12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "colonial-davis",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    \"\"\"custom function to remove the punctuation\"\"\"\n",
    "    PUNCT_TO_REMOVE = string.punctuation\n",
    "    PUNCT_TO_REMOVE += 'â’'\n",
    "    spaces = ''\n",
    "    for i in range(len(PUNCT_TO_REMOVE)):\n",
    "        spaces += ' '\n",
    "    return text.translate(str.maketrans(PUNCT_TO_REMOVE, spaces))\n",
    "df[\"OriginalTweet\"] = df[\"OriginalTweet\"].apply(lambda text: remove_punctuation(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "broad-adapter",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panic food buying germany due coronavirus begun organic left behind hamsterkauf panic buying called hamster purchaseshamsterkã¤ufeconfusion german taken way hamsters stuff cheeks food\n"
     ]
    }
   ],
   "source": [
    "print(df[\"OriginalTweet\"][12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-serum",
   "metadata": {},
   "source": [
    "### 8. Letrak eta zenbakiak zuzendu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "annual-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['OriginalTweet'] = df['OriginalTweet'].map(lambda x: re.sub('[^A-Za-z ]+', '', x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "engaged-saver",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "panic food buying germany due coronavirus begun organic left behind hamsterkauf panic buying called hamster purchaseshamsterkufeconfusion german taken way hamsters stuff cheeks food\n"
     ]
    }
   ],
   "source": [
    "print(df[\"OriginalTweet\"][12])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "appropriate-stockholm",
   "metadata": {},
   "source": [
    "### 10. Lemmanization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-hypothesis",
   "metadata": {},
   "source": [
    "Prozezu honetan, hitza bere hitz familiagaitik edo \"lema\" gaitik aldatzen da."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "removed-province",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "def lemmatize_words(text):\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "df[\"OriginalTweet\"] = df[\"OriginalTweet\"].apply(lambda text: lemmatize_words(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fresh-animation",
   "metadata": {},
   "source": [
    "### 11. Hitzak zuzendu (berriro)\n",
    "\n",
    "Hitzak (berriro) zuzentzea garrantzitsua da. Aurreko prozezua gogorra da, eta hitz batzuk apurtu daitezke, hau dela eta, berriro ere konprobatuko dugu ea hitzak ondo dauden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "laughing-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data_preprocess_long.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peaceful-technical",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
